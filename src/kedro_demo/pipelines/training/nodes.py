
import os
import pandas as pd
import numpy as np
from typing import Dict, Any

from sklearn.model_selection import GridSearchCV
from sklearn.base import BaseEstimator
from sklearn.metrics import precision_score
from sklearn.metrics import (recall_score, precision_score, f1_score,
                              accuracy_score, confusion_matrix, ConfusionMatrixDisplay)
# from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold

from typing import Callable, Tuple, Any, Dict

import mlflow
import mlflow.sklearn # Wrapper pour scikit-learn

from google.cloud import storage

from . import  model_builder

import nltk
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# Authentification à Google Cloud avec la clé correspondant au compte de service MLflow
# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.expanduser("~\OneDrive\Bureau\Lead DS\kedro-demo\conf\local\mlflow-key.json")
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.getenv("MLFLOW_CREDENTIAL_PATH")
# Set MLflow server uri
MLFLOW_SERVER = os.getenv("MLFLOW_SERVER")
mlflow.set_tracking_uri(MLFLOW_SERVER)
client = storage.Client()


def experiment(model:BaseEstimator, name:str, X_train:pd.DataFrame,
 y_train:pd.DataFrame, X_test:pd.DataFrame, y_test:pd.DataFrame):
    """
        This method is defining an experiment for a machine learning
        model. It takes in a model object, a name for the experiment,
        training and testing data, and training and testing labels.
    """

    # sets the name of the experiment using 
    # the environment variable...
    EXPERIMENT_NAME = os.getenv("EXPERIMENT_NAME")

    # ...then creates a new experiment
    mlflow.set_experiment(EXPERIMENT_NAME)

    # starts a new run within the experiment
    with mlflow.start_run(description=name) as run:

        # fits the model on the training data
        model.fit(X_train, y_train.values.ravel())

        # calculates various metrics such as F1 score,
        # accuracy, precision, and recall
        predictions = model.predict(X_test)
        f1 = f1_score(y_test, predictions, average='macro', zero_division=1)*100
        accuracy = accuracy_score(y_test, predictions)*100
        precision = precision_score(y_test, predictions, average='macro')*100
        recall = recall_score(y_test, predictions, average='macro')*100
        # save_pr_curve(X_test, y_test, model)
        # mlflow.log_text(name)

        # logs the parameters and the metrics
        # of the model
        mlflow.log_params(model.best_params_)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("precision", precision)

        # mlflow.log_artifact(os.path.expanduser("~/data/pr_curve.png"), artifact_path="plots")
        
        # logs the trained model
        mlflow.sklearn.log_model(model.best_estimator_, "model")


def get_param_grid(name:str, hpar:dict) -> dict:
    param_grid={
                f'{name}__{k}': v for (k, v) in hpar.items()
                }
    return param_grid


def train_model(
                X_train:pd.DataFrame,
                y_train:pd.DataFrame, 
                X_test:pd.DataFrame,
                y_test:pd.DataFrame,
                n_splits_StratifiedKFold:int
                                            ) -> BaseEstimator:
    """
        This method is training a machine learning model
        on a given training set and hyper-parameters. 
        It uses a list of classifiers models as generated by the 
        model_builder.generate_model_list() method.
    """

    # Generate a list of classifiers models
    models = model_builder.generate_model_list()
    f1_score_list = []
    
    
    for model in models[:]:
        
        print('\n::::::::::::::::::::::::::::::::::::::::')
        # print(text_encoding_pipeline.fit_transform(X_train))
        # print(X_train)
        # print(preprocessing.fit_transform(X_train).toarray)
        # print(y_train.shape)
        
        # StratifiedKFold cross validation
        # performs stratified k-fold cross-validation 
        # to evaluate the performance of each model 
        # on the training set. 
        # It calculates the F1 score for each fold and averages
        # them to get the overall F1 score for the model.
        skf = StratifiedKFold(n_splits=n_splits_StratifiedKFold, random_state=None, shuffle=False,)
        SK_f1_score_list = []

        
        # m = 0
        for train_index, test_index in skf.split(X_train, y_train):
            # m+=1
            # print(m)
            # print(train_index, test_index)
            x_train_fold, x_test_fold = X_train.loc[train_index], X_train.loc[test_index]
            # print(x_train_fold, x_test_fold)
            y_train_fold, y_test_fold = y_train.loc[train_index], y_train.loc[test_index]
            # print(y_train_fold, y_test_fold)
            # print(model['model'])
            # print(y_train_fold)
            # print(type(y_train_fold))

            # train the model 
            model['model'].fit(x_train_fold, y_train_fold.values.ravel())
            predictions = model['model'].predict(x_test_fold)
            f1 = f1_score(predictions, y_test_fold, average='macro', zero_division=1)
            SK_f1_score_list.append(f1)
        

        average_score = np.mean(SK_f1_score_list)
        f1_score_list.append((average_score, model))

        print(f1_score_list)
        print('::::::::::::::::::::::::::::::::::::::::\n')
        # print(SK_f1_score_list)
        # print(f1_score_list)

        # hpar = get_param_grid(model['name'], model['hpar'])
        # clf = GridSearchCV(model['model'], hpar)
        # experiment(clf,model['name'], X_train, y_train, X_test, y_test)

    # get best model from cross validation
    f1_score_list.sort(reverse=True)
    best_model = f1_score_list[0][1]
    # print(best_model)

    # uses grid search to find the best hyper-parameters
    # for the best model
    hpar = get_param_grid(best_model['name'], best_model['hpar'])
    clf = GridSearchCV(best_model['model'], hpar)

    # trains and test the model on the entire training set
    experiment(clf,'BEST ' + best_model['name'], X_train, y_train, X_test, y_test)

    # returns the best model
    return clf
